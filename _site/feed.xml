<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2018-09-05T11:02:58-04:00</updated><id>http://localhost:4000/</id><title type="html">Parker Lawrence portfolio</title><subtitle>Parker Lawrence's portfolio website and blog about interesting topics. If you're working on similar projects or you know Parker Lawrence, you may find this site interesting.</subtitle><entry><title type="html">Breadboard computer operations</title><link href="http://localhost:4000/ted/2018/06/11/minmax-tree.html" rel="alternate" type="text/html" title="Breadboard computer operations" /><published>2018-06-11T10:39:54-04:00</published><updated>2018-06-11T10:39:54-04:00</updated><id>http://localhost:4000/ted/2018/06/11/minmax-tree</id><content type="html" xml:base="http://localhost:4000/ted/2018/06/11/minmax-tree.html">&lt;p&gt;
The other component to the chess AI is a minmax tree. This is what allows the program to look ahead to see which moves put it in a better position. When the program looks ahead some number of moves in advance, it constructs a tree that represents the next possible moves. After a certain number of moves in the future, the program will cap the tree off with the heuristic value of that node. It is assumed that the program will choose the best possible path for it, and the opponent will choose the worst path for the program. The heuristic for each node that is not a terminal node will be the max or min of its children respectively.
&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/ted/chapter2/figure1.png&quot; alt=&quot;figure 1&quot; /&gt;&lt;/p&gt;
&lt;p&gt;
Consider the below diagram, where the tree is in the middle of being calculated. One of the paths does not need to be explored, because the only effect it could have on the value of its parent heuristic is to lower it, and its parent’s heuristic is already too low to be considered. It can be skipped. This process is known as alpha-beta pruning, and it eliminates a lot of computation needed, which allows the program to look further ahead. In general, anytime the maximum score that the minimizing player is assured of is greater than or equal to the minimum score that the maximizing player is assured of, that node does not need to be explored. 
&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/ted/chapter2/figure2.png&quot; alt=&quot;figure 2&quot; /&gt;&lt;/p&gt;
&lt;p&gt;
Each heuristic is expensive to calculate because it involves evaluating a neural network over the chess board. I had to play around with what inputs the neural network received to make it as effective as possible. I started with simply giving each square of the board an input for each piece it could possibly have. This was not feasible because of the number of nodes needed. 
&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/ted/chapter2/figure3.png&quot; alt=&quot;figure 3&quot; /&gt;&lt;/p&gt;
&lt;p&gt;
The second method I tried was using a convolutional neural network. A convolutional neural network re-uses groups of weights in a moving window over the input, which must be arranged in a grid. 
&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/ted/chapter2/figure4.png&quot; alt=&quot;figure 4&quot; /&gt;&lt;/p&gt;
&lt;p&gt;
After experimenting with various convolution sizes and approaches, the best solution turned out to be adding another input to the neural network for which pieces were under pressure, or resting on a space that could be attacked by one of the opponents pieces. After that, smaller convolutions were much more effective because they trained faster and executed faster, allowing the program to look further ahead. The solution that reigned supreme was to use a neural network with an input for each kind of piece that could lie on that space, an input for if that space was under pressure, and an input for if that space were guarded. The neural network is evaluated for each square in the grid, and summed up to get the heuristic of the entire grid. 
&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/ted/chapter2/figure5.png&quot; alt=&quot;figure 5&quot; /&gt;&lt;/p&gt;
&lt;p&gt;
Once a game has completed, the neural network is trained over the last 8 games. The last game has the highest learning rate, with the learning rate tapering off by the 8th to last game, because it is easiest to determine a game’s outcome by later board positions. The program was trained by both playing against itself and also from games collected with a webscraper. 
&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/ted/chapter2/figure6.png&quot; alt=&quot;figure 6&quot; /&gt;&lt;/p&gt;

&lt;div&gt;


&lt;a href=&quot;/ted/2018/06/04/neural-networks.html&quot; class=&quot;float:left;&quot;&gt;&amp;#8249; Neural networks&lt;/a&gt;

&lt;/div&gt;</content><author><name></name></author><summary type="html">The other component to the chess AI is a minmax tree. This is what allows the program to look ahead to see which moves put it in a better position. When the program looks ahead some number of moves in advance, it constructs a tree that represents the next possible moves. After a certain number of moves in the future, the program will cap the tree off with the heuristic value of that node. It is assumed that the program will choose the best possible path for it, and the opponent will choose the worst path for the program. The heuristic for each node that is not a terminal node will be the max or min of its children respectively. Consider the below diagram, where the tree is in the middle of being calculated. One of the paths does not need to be explored, because the only effect it could have on the value of its parent heuristic is to lower it, and its parent’s heuristic is already too low to be considered. It can be skipped. This process is known as alpha-beta pruning, and it eliminates a lot of computation needed, which allows the program to look further ahead. In general, anytime the maximum score that the minimizing player is assured of is greater than or equal to the minimum score that the maximizing player is assured of, that node does not need to be explored. Each heuristic is expensive to calculate because it involves evaluating a neural network over the chess board. I had to play around with what inputs the neural network received to make it as effective as possible. I started with simply giving each square of the board an input for each piece it could possibly have. This was not feasible because of the number of nodes needed. The second method I tried was using a convolutional neural network. A convolutional neural network re-uses groups of weights in a moving window over the input, which must be arranged in a grid. After experimenting with various convolution sizes and approaches, the best solution turned out to be adding another input to the neural network for which pieces were under pressure, or resting on a space that could be attacked by one of the opponents pieces. After that, smaller convolutions were much more effective because they trained faster and executed faster, allowing the program to look further ahead. The solution that reigned supreme was to use a neural network with an input for each kind of piece that could lie on that space, an input for if that space was under pressure, and an input for if that space were guarded. The neural network is evaluated for each square in the grid, and summed up to get the heuristic of the entire grid. Once a game has completed, the neural network is trained over the last 8 games. The last game has the highest learning rate, with the learning rate tapering off by the 8th to last game, because it is easiest to determine a game’s outcome by later board positions. The program was trained by both playing against itself and also from games collected with a webscraper.</summary></entry><entry><title type="html">Neural networks</title><link href="http://localhost:4000/ted/2018/06/04/neural-networks.html" rel="alternate" type="text/html" title="Neural networks" /><published>2018-06-04T10:39:54-04:00</published><updated>2018-06-04T10:39:54-04:00</updated><id>http://localhost:4000/ted/2018/06/04/neural-networks</id><content type="html" xml:base="http://localhost:4000/ted/2018/06/04/neural-networks.html">&lt;p&gt;
Stochastic gradient descent refers to making a function more accurate by adjusting its weights according to the derivative of the function’s error with respect to each weight. 
&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/ted/chapter1/figure1.png&quot; alt=&quot;figure 1&quot; /&gt;&lt;/p&gt;
&lt;p&gt;
The more weights the function has, the more flexible the function can be. A good way to make a function with a lot of weights that can be executed quickly is to arrange the weights as a series of matrix transformations. 
&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/ted/chapter1/figure2.png&quot; alt=&quot;figure 2&quot; /&gt;&lt;/p&gt;
&lt;p&gt;
The resulting function will always be a linear model, which is very limited and could be solved with a linear regression method anyway. The solution is to apply a non-linear function to each vector in between matrices. This non-linear function is called an activation function. Many different activation functions are used, such as inverse tangent or a logistic function. The shape of the function is more important than its value.  
&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/ted/chapter1/figure3.png&quot; alt=&quot;figure 3&quot; /&gt;&lt;/p&gt;
&lt;p&gt;
The above steps result in a neural network. It’s important to note that the most important parts of a neural network are not biomimetic. In my TED talk I liken the neural network to the way humans learn because it’s a good analogy, but the similarities aren’t the reason neural networks are able to work effectively. They work effectively because of the reasons stated above.
&lt;/p&gt;

&lt;div&gt;

&lt;a href=&quot;/ted/2018/06/11/minmax-tree.html&quot; style=&quot;float:right;&quot;&gt;Breadboard computer operations &amp;#8250;&lt;/a&gt;


&lt;/div&gt;</content><author><name></name></author><summary type="html">Stochastic gradient descent refers to making a function more accurate by adjusting its weights according to the derivative of the function’s error with respect to each weight. The more weights the function has, the more flexible the function can be. A good way to make a function with a lot of weights that can be executed quickly is to arrange the weights as a series of matrix transformations. The resulting function will always be a linear model, which is very limited and could be solved with a linear regression method anyway. The solution is to apply a non-linear function to each vector in between matrices. This non-linear function is called an activation function. Many different activation functions are used, such as inverse tangent or a logistic function. The shape of the function is more important than its value. The above steps result in a neural network. It’s important to note that the most important parts of a neural network are not biomimetic. In my TED talk I liken the neural network to the way humans learn because it’s a good analogy, but the similarities aren’t the reason neural networks are able to work effectively. They work effectively because of the reasons stated above.</summary></entry><entry><title type="html">Breadboard computer operations</title><link href="http://localhost:4000/proof/2018/06/01/writing-a-general-parser.html" rel="alternate" type="text/html" title="Breadboard computer operations" /><published>2018-06-01T10:39:54-04:00</published><updated>2018-06-01T10:39:54-04:00</updated><id>http://localhost:4000/proof/2018/06/01/writing-a-general-parser</id><content type="html" xml:base="http://localhost:4000/proof/2018/06/01/writing-a-general-parser.html">&lt;p&gt;
OOPS! This page is still being worked on.
&lt;/p&gt;

&lt;div&gt;


&lt;a href=&quot;/proof/2018/05/31/proof-engine-solver-algorithm.html&quot; class=&quot;float:left;&quot;&gt;&amp;#8249; Proof engine solver algorithm&lt;/a&gt;

&lt;/div&gt;</content><author><name></name></author><summary type="html">OOPS! This page is still being worked on.</summary></entry><entry><title type="html">Proof engine solver algorithm</title><link href="http://localhost:4000/proof/2018/05/31/proof-engine-solver-algorithm.html" rel="alternate" type="text/html" title="Proof engine solver algorithm" /><published>2018-05-31T10:39:54-04:00</published><updated>2018-05-31T10:39:54-04:00</updated><id>http://localhost:4000/proof/2018/05/31/proof-engine-solver-algorithm</id><content type="html" xml:base="http://localhost:4000/proof/2018/05/31/proof-engine-solver-algorithm.html">&lt;p&gt;
The most difficult concept to grasp in type theory is that predicates are types. The image below shows the signatures of some examples. Predicates/types may be true, false, or unprovable, but what proves a predicate to be true is if it is inhabited. In other words, if there is a valid statement whose type is predicate P, then P is proven, and the statement is its proof. 
&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/proof/chapter4/figure1.png&quot; alt=&quot;figure 1&quot; /&gt;&lt;/p&gt;
&lt;p&gt;
Targets are like signatures- they have a set of children which are signatures and may refer to each other. There are tricks for representing theorems as targets.
&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/proof/chapter4/figure2.png&quot; alt=&quot;figure 2&quot; /&gt;&lt;/p&gt;
&lt;p&gt;
The goal of the theorem prover, then, is to find such a statement. A naive algorithm would start with its set of axioms and prove more and more predicates, expanding its ‘truth set’ until it happens to encompass the target theorem. A much more efficient approach is to work backwards instead, expanding the finite set of predicates which imply the target theorem until the set contains a statement known to be true. This approach is more focused, and has more features upon which to speed up the search. Some elements in the predicate set obsolete each other, so many of these elements can be removed during a search to keep the search space in check.
&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/proof/chapter4/figure3.png&quot; alt=&quot;figure 3&quot; /&gt;&lt;/p&gt;
&lt;p&gt;
Another way to think of the problem is to set up an unknown variable whose type signature is the target predicate. The type signature may also have arguments, which allow universal quantification and preconditions. That unknown variable’s root node is either an axiom, or one of its parameters. The unknown variable can be inserted as a pair with some special rules for each possibility, with the unknown variable on the left and the possibility on the right. The first rule is that each parameter that the right hand node accepts is a new unknown variable. Each of those unknown variables’ signatures are the same as the root node’s corresponding argument, but with the left hand node’s argument signatures tacked on as extra arguments. In addition to being added as pairs, their types must be added as pairs as well. (Which is the key part of the algorithm, because it asserts that the types are judgmentally equal, ensuring that the proof is valid.) The below diagram shows one step. 
&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/proof/chapter4/figure4.png&quot; alt=&quot;figure 4&quot; /&gt;&lt;/p&gt;
&lt;p&gt;
As you can see from that diagram, some of the unknowns don’t make sense to solve for. The general rule is that if an unknown’s substitution variable is referenced by another unknown’s signature, it should not be solved for. The exception is in the case of circular loops, because then there can be a situation where no unknown meets the criteria. Reference loops should usually be avoided, but as long as all axioms and targets are well-formed and consistent, they cause no problems with the algorithm. (If the input is valid, then any condition that would cause the algorithm to do an infinite substitution would sooner cause the algorithm to return ‘no solution’. And not as a cheap workaround either- there is strictly no solution in that case.)
After the algorithm makes a bunch of binding objects representing attempts at solving one of the unknowns with an axiom, each binding object is simplified and split into solved forms. Some binding objects will fail, which means that axiom cannot be used to satisfy that unknown. Other binding objects will split into many solutions, meaning there is more than one way to apply that axiom in that case. Each application of an axiom may create more unknowns that need to be solved for, and it may coincidentally specify unknowns besides the one being solved for. 
&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/proof/chapter4/figure5.png&quot; alt=&quot;figure 5&quot; /&gt;&lt;/p&gt;

&lt;div&gt;

&lt;a href=&quot;/proof/2018/06/01/writing-a-general-parser.html&quot; style=&quot;float:right;&quot;&gt;Breadboard computer operations &amp;#8250;&lt;/a&gt;


&lt;a href=&quot;/proof/2018/05/30/higher-order-unification.html&quot; class=&quot;float:left;&quot;&gt;&amp;#8249; Higher order unification&lt;/a&gt;

&lt;/div&gt;</content><author><name></name></author><summary type="html">The most difficult concept to grasp in type theory is that predicates are types. The image below shows the signatures of some examples. Predicates/types may be true, false, or unprovable, but what proves a predicate to be true is if it is inhabited. In other words, if there is a valid statement whose type is predicate P, then P is proven, and the statement is its proof. Targets are like signatures- they have a set of children which are signatures and may refer to each other. There are tricks for representing theorems as targets. The goal of the theorem prover, then, is to find such a statement. A naive algorithm would start with its set of axioms and prove more and more predicates, expanding its ‘truth set’ until it happens to encompass the target theorem. A much more efficient approach is to work backwards instead, expanding the finite set of predicates which imply the target theorem until the set contains a statement known to be true. This approach is more focused, and has more features upon which to speed up the search. Some elements in the predicate set obsolete each other, so many of these elements can be removed during a search to keep the search space in check. Another way to think of the problem is to set up an unknown variable whose type signature is the target predicate. The type signature may also have arguments, which allow universal quantification and preconditions. That unknown variable’s root node is either an axiom, or one of its parameters. The unknown variable can be inserted as a pair with some special rules for each possibility, with the unknown variable on the left and the possibility on the right. The first rule is that each parameter that the right hand node accepts is a new unknown variable. Each of those unknown variables’ signatures are the same as the root node’s corresponding argument, but with the left hand node’s argument signatures tacked on as extra arguments. In addition to being added as pairs, their types must be added as pairs as well. (Which is the key part of the algorithm, because it asserts that the types are judgmentally equal, ensuring that the proof is valid.) The below diagram shows one step. As you can see from that diagram, some of the unknowns don’t make sense to solve for. The general rule is that if an unknown’s substitution variable is referenced by another unknown’s signature, it should not be solved for. The exception is in the case of circular loops, because then there can be a situation where no unknown meets the criteria. Reference loops should usually be avoided, but as long as all axioms and targets are well-formed and consistent, they cause no problems with the algorithm. (If the input is valid, then any condition that would cause the algorithm to do an infinite substitution would sooner cause the algorithm to return ‘no solution’. And not as a cheap workaround either- there is strictly no solution in that case.) After the algorithm makes a bunch of binding objects representing attempts at solving one of the unknowns with an axiom, each binding object is simplified and split into solved forms. Some binding objects will fail, which means that axiom cannot be used to satisfy that unknown. Other binding objects will split into many solutions, meaning there is more than one way to apply that axiom in that case. Each application of an axiom may create more unknowns that need to be solved for, and it may coincidentally specify unknowns besides the one being solved for.</summary></entry><entry><title type="html">Higher order unification</title><link href="http://localhost:4000/proof/2018/05/30/higher-order-unification.html" rel="alternate" type="text/html" title="Higher order unification" /><published>2018-05-30T10:39:54-04:00</published><updated>2018-05-30T10:39:54-04:00</updated><id>http://localhost:4000/proof/2018/05/30/higher-order-unification</id><content type="html" xml:base="http://localhost:4000/proof/2018/05/30/higher-order-unification.html">&lt;p&gt;
This article is on the way. It's a very complicated topic.
&lt;/p&gt;

&lt;div&gt;

&lt;a href=&quot;/proof/2018/05/31/proof-engine-solver-algorithm.html&quot; style=&quot;float:right;&quot;&gt;Proof engine solver algorithm &amp;#8250;&lt;/a&gt;


&lt;a href=&quot;/proof/2018/05/29/proof-engine-operations.html&quot; class=&quot;float:left;&quot;&gt;&amp;#8249; Proof engine operations&lt;/a&gt;

&lt;/div&gt;</content><author><name></name></author><summary type="html">This article is on the way. It's a very complicated topic.</summary></entry><entry><title type="html">Proof engine operations</title><link href="http://localhost:4000/proof/2018/05/29/proof-engine-operations.html" rel="alternate" type="text/html" title="Proof engine operations" /><published>2018-05-29T10:39:54-04:00</published><updated>2018-05-29T10:39:54-04:00</updated><id>http://localhost:4000/proof/2018/05/29/proof-engine-operations</id><content type="html" xml:base="http://localhost:4000/proof/2018/05/29/proof-engine-operations.html">&lt;p&gt;
The proof algorithm involves performing a process similar to algebra, but with judgmental equality. When this kind of algebra is done, an object is created to store all restrictions placed on the unknown variables. The object stores the scope, which contains a reference to the signatures of all axioms usable by the machine in the first row, and the signatures of each unknown variable in the second. The object also stores some number of pairs of statements known to be judgmentally equal. Each pair of statements also has a scope associated with it for variables that are neither axioms nor unknown variables but are present in that pair of statements. I call the object a binding object, because it binds statements together so that they must be judgmentally equivalent.
&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/proof/chapter2/figure1.png&quot; alt=&quot;figure 1&quot; /&gt;&lt;/p&gt;
&lt;p&gt;
Simplifying binding objects provides information about the unknown variables that it operates on. The proof deducer is interested in simplifying binding objects until no more information can be extracted from them. The most basic form of simplification is recursively comparing both sides of each pair- if the two root nodes differ- either one of the root nodes is an unknown variable or there is no solution to the binding object and it must be destroyed. This often results in variables being created in the pair’s scope. After this process, all pairs consist of at least one structure whose root node is an unknown variable.
&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/proof/chapter2/figure2.png&quot; alt=&quot;figure 2&quot; /&gt;&lt;/p&gt;
&lt;p&gt;
The second process is identifying which patterns can be substituted into other patterns. Substituting both sides of each pair and then going back to step one of the simplification process typically eliminates the target pair and creates more useful pairs from it.
&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/proof/chapter2/figure3.png&quot; alt=&quot;figure 3&quot; /&gt;&lt;/p&gt;
&lt;p&gt;
In some cases, binding objects must be split into multiple different cases and simplified separately. The objective of this next algorithm is to handle cases like the one illustrated below, and turn them into more useful pairs. Essentially, the algorithm performs pattern matching and returns each possible combination.
&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/proof/chapter2/figure4.png&quot; alt=&quot;figure 4&quot; /&gt;&lt;/p&gt;
&lt;p&gt;
This problem is known as higher order unification. The next chapter focuses solely on it, because it can be a very difficult problem.
&lt;/p&gt;

&lt;div&gt;

&lt;a href=&quot;/proof/2018/05/30/higher-order-unification.html&quot; style=&quot;float:right;&quot;&gt;Higher order unification &amp;#8250;&lt;/a&gt;


&lt;a href=&quot;/proof/2018/05/28/proof-engine-structures.html&quot; class=&quot;float:left;&quot;&gt;&amp;#8249; Proof Engine structures&lt;/a&gt;

&lt;/div&gt;</content><author><name></name></author><summary type="html">The proof algorithm involves performing a process similar to algebra, but with judgmental equality. When this kind of algebra is done, an object is created to store all restrictions placed on the unknown variables. The object stores the scope, which contains a reference to the signatures of all axioms usable by the machine in the first row, and the signatures of each unknown variable in the second. The object also stores some number of pairs of statements known to be judgmentally equal. Each pair of statements also has a scope associated with it for variables that are neither axioms nor unknown variables but are present in that pair of statements. I call the object a binding object, because it binds statements together so that they must be judgmentally equivalent. Simplifying binding objects provides information about the unknown variables that it operates on. The proof deducer is interested in simplifying binding objects until no more information can be extracted from them. The most basic form of simplification is recursively comparing both sides of each pair- if the two root nodes differ- either one of the root nodes is an unknown variable or there is no solution to the binding object and it must be destroyed. This often results in variables being created in the pair’s scope. After this process, all pairs consist of at least one structure whose root node is an unknown variable. The second process is identifying which patterns can be substituted into other patterns. Substituting both sides of each pair and then going back to step one of the simplification process typically eliminates the target pair and creates more useful pairs from it. In some cases, binding objects must be split into multiple different cases and simplified separately. The objective of this next algorithm is to handle cases like the one illustrated below, and turn them into more useful pairs. Essentially, the algorithm performs pattern matching and returns each possible combination. This problem is known as higher order unification. The next chapter focuses solely on it, because it can be a very difficult problem.</summary></entry><entry><title type="html">Proof Engine structures</title><link href="http://localhost:4000/proof/2018/05/28/proof-engine-structures.html" rel="alternate" type="text/html" title="Proof Engine structures" /><published>2018-05-28T10:39:54-04:00</published><updated>2018-05-28T10:39:54-04:00</updated><id>http://localhost:4000/proof/2018/05/28/proof-engine-structures</id><content type="html" xml:base="http://localhost:4000/proof/2018/05/28/proof-engine-structures.html">&lt;p&gt;
	TO DO: I'm going to include visualizations from my gephi visualizer.
The most basic building blocks in the proof deducer are statements. I define statements to be a kind of tree. Each node in the tree is a constant, some kind of operation, or a substitution variable. Note that statements are not necessarily numerical- they can have much more complex types than that (the type of a statement can also be represented as a statement). Examples include a-b, f(a,b,c), f(a+b-c,d,e).
&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/proof/chapter1/figure1.png&quot; alt=&quot;figure 1&quot; /&gt;&lt;/p&gt;
&lt;p&gt;
A node’s type, number of arguments, and the types of its arguments are determined by a signature, which I define. Like statements, signatures are also trees, but with key differences. When a node appears in a statement, it’s like a function call. Signatures are like function definitions.
&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/proof/chapter1/figure2.png&quot; alt=&quot;figure 2&quot; /&gt;&lt;/p&gt;
&lt;p&gt;
Functions may also be dependently typed. A signature’s type or the signatures of its arguments may be written in terms of substitution variables standing in for its parameters. In other words, dependently typed function have a type or arguments whose types depend on the function’s inputs. A powerful example of a dependently typed function is the reflexivity property, but to completely understand it, you’d either need some prior type theory knowledge or to read a few more chapters of this blog.
&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/proof/chapter1/figure3.png&quot; alt=&quot;figure 3&quot; /&gt;&lt;/p&gt;
&lt;p&gt;
Since signatures are recursive, parameters may also be functions. To pass a function to a function, function variables may be referenced within the argument that expects a function as a parameter. This is difficult to describe in words, so the figure below shows this in greater detail. In the following example, the ‘derivative at a point’ function’s signature is shown. It accepts two parameters- the function to differentiate, and the input to the derivative. From this, the second derivative of a function can be constructed.
&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/proof/chapter1/figure4.png&quot; alt=&quot;figure 4&quot; /&gt;&lt;/p&gt;
&lt;p&gt;
Each node may only reference signatures within its scope. When a statement is interpreted for any purpose, it is done so recursively, so that scope can be built on the way down. Scope takes the form of a staggered 2d array of signatures, so that each move down the tree adds another row to the scope. Nodes index into the scope with a pair of integers. The first row in the scope is always the complete set of type theory axioms that the proof deducer must build everything out of.
&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/proof/chapter1/figure5.png&quot; alt=&quot;figure 5&quot; /&gt;&lt;/p&gt;
&lt;p&gt;
Signatures are also interpreted with a scope. Because (in the case of dependently typed functions) each child signature or the type statement may reference any of the signature’s arguments as substitution variables, a signature adds its child signatures as a new row to the scope as it is interpreted.
&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/proof/chapter1/figure6.png&quot; alt=&quot;figure 6&quot; /&gt;&lt;/p&gt;
&lt;p&gt;
The proof deducer can be very difficult to debug. To sanitize inputs and ensure good output, statements must be type checked. Type checking involves using a signature to generate a statement’s type, and then compare the generated type with the expected type for that statement. The expected types of the node’s arguments are then generated and then the type checking is continued recursively. The proof deducer obsessively type checks all statements during development because it helps catch bugs.
&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/proof/chapter1/figure7.png&quot; alt=&quot;figure 7&quot; /&gt;&lt;/p&gt;
&lt;p&gt;
In type theory, two statements are judgmentally equal if and only if their structures are the same. Under this definition, 3+2 is judgmentally equivalent to 3+2, but not 2+3 or 5. When the program performs type checking for the purpose of sanitizing inputs or debugging, the expected type and the generated type are compared to see if they are judgmentally equal. If they are not, the program throws an exception. In other areas of the program, type checking is performed with different goals, such as setting the expected type equal to the generated type to solve for unknowns.
&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/proof/chapter1/figure8.png&quot; alt=&quot;figure 8&quot; /&gt;&lt;/p&gt;

&lt;div&gt;

&lt;a href=&quot;/proof/2018/05/29/proof-engine-operations.html&quot; style=&quot;float:right;&quot;&gt;Proof engine operations &amp;#8250;&lt;/a&gt;


&lt;/div&gt;</content><author><name></name></author><summary type="html">TO DO: I'm going to include visualizations from my gephi visualizer. The most basic building blocks in the proof deducer are statements. I define statements to be a kind of tree. Each node in the tree is a constant, some kind of operation, or a substitution variable. Note that statements are not necessarily numerical- they can have much more complex types than that (the type of a statement can also be represented as a statement). Examples include a-b, f(a,b,c), f(a+b-c,d,e). A node’s type, number of arguments, and the types of its arguments are determined by a signature, which I define. Like statements, signatures are also trees, but with key differences. When a node appears in a statement, it’s like a function call. Signatures are like function definitions. Functions may also be dependently typed. A signature’s type or the signatures of its arguments may be written in terms of substitution variables standing in for its parameters. In other words, dependently typed function have a type or arguments whose types depend on the function’s inputs. A powerful example of a dependently typed function is the reflexivity property, but to completely understand it, you’d either need some prior type theory knowledge or to read a few more chapters of this blog. Since signatures are recursive, parameters may also be functions. To pass a function to a function, function variables may be referenced within the argument that expects a function as a parameter. This is difficult to describe in words, so the figure below shows this in greater detail. In the following example, the ‘derivative at a point’ function’s signature is shown. It accepts two parameters- the function to differentiate, and the input to the derivative. From this, the second derivative of a function can be constructed. Each node may only reference signatures within its scope. When a statement is interpreted for any purpose, it is done so recursively, so that scope can be built on the way down. Scope takes the form of a staggered 2d array of signatures, so that each move down the tree adds another row to the scope. Nodes index into the scope with a pair of integers. The first row in the scope is always the complete set of type theory axioms that the proof deducer must build everything out of. Signatures are also interpreted with a scope. Because (in the case of dependently typed functions) each child signature or the type statement may reference any of the signature’s arguments as substitution variables, a signature adds its child signatures as a new row to the scope as it is interpreted. The proof deducer can be very difficult to debug. To sanitize inputs and ensure good output, statements must be type checked. Type checking involves using a signature to generate a statement’s type, and then compare the generated type with the expected type for that statement. The expected types of the node’s arguments are then generated and then the type checking is continued recursively. The proof deducer obsessively type checks all statements during development because it helps catch bugs. In type theory, two statements are judgmentally equal if and only if their structures are the same. Under this definition, 3+2 is judgmentally equivalent to 3+2, but not 2+3 or 5. When the program performs type checking for the purpose of sanitizing inputs or debugging, the expected type and the generated type are compared to see if they are judgmentally equal. If they are not, the program throws an exception. In other areas of the program, type checking is performed with different goals, such as setting the expected type equal to the generated type to solve for unknowns.</summary></entry><entry><title type="html">Breadboard computer pictures</title><link href="http://localhost:4000/breadboard/2018/05/14/breadboard-computer-pictures.html" rel="alternate" type="text/html" title="Breadboard computer pictures" /><published>2018-05-14T10:39:54-04:00</published><updated>2018-05-14T10:39:54-04:00</updated><id>http://localhost:4000/breadboard/2018/05/14/breadboard-computer-pictures</id><content type="html" xml:base="http://localhost:4000/breadboard/2018/05/14/breadboard-computer-pictures.html">&lt;p&gt;
Here's where the pictures would go. This page is under construction.
&lt;/p&gt;

&lt;div&gt;


&lt;a href=&quot;/breadboard/2018/05/07/custom-language-compiler.html&quot; class=&quot;float:left;&quot;&gt;&amp;#8249; Custom Language Compiler&lt;/a&gt;

&lt;/div&gt;</content><author><name></name></author><summary type="html">Here's where the pictures would go. This page is under construction.</summary></entry><entry><title type="html">Custom Language Compiler</title><link href="http://localhost:4000/breadboard/2018/05/07/custom-language-compiler.html" rel="alternate" type="text/html" title="Custom Language Compiler" /><published>2018-05-07T10:39:54-04:00</published><updated>2018-05-07T10:39:54-04:00</updated><id>http://localhost:4000/breadboard/2018/05/07/custom-language-compiler</id><content type="html" xml:base="http://localhost:4000/breadboard/2018/05/07/custom-language-compiler.html">&lt;p&gt;
This computer cannot accept normal instructions, which means I had to write a compiler that can produce the special kind of assembly language that the computer can execute. Rather than compile an existing language, I made a very barebones language that I named K. The language is first tokenized, and then interpreted into an intermediate structure which can be very quickly turned into my assembly. For more about parsing, see the proof software project. 
&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/breadboard/chapter4/figure1.png&quot; alt=&quot;figure 1&quot; /&gt;&lt;/p&gt;
&lt;p&gt;
The intermediate structure representation of the program is a tree. For this simplified language, there are expression nodes and structure nodes. The root node of the program is a structure node. Structure nodes are for blocks of code such as loops, conditionals, variable declarations, assignments, and functions. Expression nodes are for connectives within an expression such as addition, subtraction, variable reference, and the like.
&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;
	Variable declarations are the simplest structure node. They are made to chain together and do not become assembly language, only influence the stack, which changes the way later structures are parsed. 
	&lt;img src=&quot;http://localhost:4000/assets/breadboard/chapter4/figure2.png&quot; /&gt;
	&lt;/li&gt;
	&lt;li&gt;
	Assignments are the next structure node. I make assignment a structure rather than an expression to simplify the language. They translate very simply- they just evaluate the expressions they contain into the variable. 
	&lt;img src=&quot;http://localhost:4000/assets/breadboard/chapter4/figure3.png&quot; /&gt;
	&lt;/li&gt;
	&lt;li&gt;
	Conditionals are simple, too. First, they evaluate their expression into the C buffer. Then, they generate an SKP instruction for the C buffer followed by a SET instruction to the instruction counter (essentially a JMP). Then, the conditional’s body is evaluated. Now that the number of instructions that the body evaluates to is known, the JMP instruction is revisited to set its destination. 
	&lt;img src=&quot;http://localhost:4000/assets/breadboard/chapter4/figure4.png&quot; /&gt;
	&lt;/li&gt;
	&lt;li&gt;
	Loops are similar to conditionals. They have another JMP instruction added after the body instructions that sends the counter back to just before the expression is evaluated. 
	&lt;img src=&quot;http://localhost:4000/assets/breadboard/chapter4/figure5.png&quot; /&gt;
	&lt;/li&gt;
	&lt;li&gt;
	Function declarations turn into a JMP instruction to ensure that the function doesn’t run when it is declared, and then each parameter turns into a variable declaration on the stack. When the function is called, any arguments will be evaluated into the first words onto the stack. 
	&lt;img src=&quot;http://localhost:4000/assets/breadboard/chapter4/figure6.png&quot; /&gt;
	&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;
As opposed to structure nodes, expression nodes are given locations to leave the expression result, be it a buffer or a location in RAM. This is how assignments work; all they do is pass the variable’s location along to their expressions. When the location is in RAM, the program assumes that the address buffer will be left at that location after the expression finishes.
&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;
	Constants are the simplest expression nodes. They turn into SET instructions. 
	&lt;img src=&quot;http://localhost:4000/assets/breadboard/chapter4/figure7.png&quot; /&gt;
	&lt;/li&gt;
	&lt;li&gt;
	Variables are much more complicated. Each time the program wants to access a variable on the stack, it must add the stack offset to the variable’s position relative to it, and set the address buffer to the result. When looking at the instructions generated by my compiler, most of the computer’s work happens to be shuffling buffers for this purpose. If I were building the computer over again, I’d change the address buffer to be two buffers connected by a full adder. That way, one buffer could contain the stack offset, cutting out ~70% of each program’s instructions. Allowing one of those buffers to be read from would improve it even more, providing a more elegant way to increase or decrease the stack offset. 
	&lt;img src=&quot;http://localhost:4000/assets/breadboard/chapter4/figure8.png&quot; /&gt;
	&lt;/li&gt;
	&lt;li&gt;
	While parsing binary expressions, an additional offset on top of the stack offset is incremented to allow words to be temporarily stored in memory, if they need to be. The following flowchart describes the optimizations done to a binary expression (if possible) 
	&lt;img src=&quot;http://localhost:4000/assets/breadboard/chapter4/figure9.png&quot; /&gt;
	&lt;/li&gt;
	&lt;li&gt;
	The program also introduces two operators for clamping values from 16 bit integer range to a 1 or 0. Essentially, a ‘truthiness’ test, and an inverse truthiness test.
	&lt;img src=&quot;http://localhost:4000/assets/breadboard/chapter4/figure10.png&quot; /&gt;
	&lt;/li&gt;
	&lt;li&gt;
	The program also supports reference and dereference operators. Pointers are the same data type as any other word, and there are no protections to ensure that the machine doesn’t change memory in use by the OS. Reference operators are are turned into constants. 
	&lt;img src=&quot;http://localhost:4000/assets/breadboard/chapter4/figure11.png&quot; /&gt;
	&lt;/li&gt;
	&lt;li&gt;
	Function calls must add onto the stack offset, leave the current position in instructions in ram, and then evaluate each of its parameter expressions into the stack variables they correspond to inside the function’s scope. After that, the function jumps to the function’s position in the instructions. Once the function reaches its return statement, it subtracts from the stack and leaves the function’s return value in the C buffer. 
	&lt;img src=&quot;http://localhost:4000/assets/breadboard/chapter4/figure12.png&quot; /&gt;
	&lt;/li&gt;
	&lt;li&gt;
	I was originally going to implement a heap with memory allocation and reallocation operators, but I realized that each one would turn into a lot of instructions and ram usage. The computer’s ALU occasionally mishandles operations, so small programs will execute fine, but any program over 80 operations isn’t worth trying. I spent a long time trying to fix that problem, because it would allow me to create a whole OS for the system, but the project got pushed to the wayside when summer ended. 
	&lt;/li&gt;
&lt;/ul&gt;

&lt;div&gt;

&lt;a href=&quot;/breadboard/2018/05/14/breadboard-computer-pictures.html&quot; style=&quot;float:right;&quot;&gt;Breadboard computer pictures &amp;#8250;&lt;/a&gt;


&lt;a href=&quot;/breadboard/2018/04/30/breadboard-computer-alu.html&quot; class=&quot;float:left;&quot;&gt;&amp;#8249; Breadboard computer ALU&lt;/a&gt;

&lt;/div&gt;</content><author><name></name></author><summary type="html">This computer cannot accept normal instructions, which means I had to write a compiler that can produce the special kind of assembly language that the computer can execute. Rather than compile an existing language, I made a very barebones language that I named K. The language is first tokenized, and then interpreted into an intermediate structure which can be very quickly turned into my assembly. For more about parsing, see the proof software project. The intermediate structure representation of the program is a tree. For this simplified language, there are expression nodes and structure nodes. The root node of the program is a structure node. Structure nodes are for blocks of code such as loops, conditionals, variable declarations, assignments, and functions. Expression nodes are for connectives within an expression such as addition, subtraction, variable reference, and the like. Variable declarations are the simplest structure node. They are made to chain together and do not become assembly language, only influence the stack, which changes the way later structures are parsed. Assignments are the next structure node. I make assignment a structure rather than an expression to simplify the language. They translate very simply- they just evaluate the expressions they contain into the variable. Conditionals are simple, too. First, they evaluate their expression into the C buffer. Then, they generate an SKP instruction for the C buffer followed by a SET instruction to the instruction counter (essentially a JMP). Then, the conditional’s body is evaluated. Now that the number of instructions that the body evaluates to is known, the JMP instruction is revisited to set its destination. Loops are similar to conditionals. They have another JMP instruction added after the body instructions that sends the counter back to just before the expression is evaluated. Function declarations turn into a JMP instruction to ensure that the function doesn’t run when it is declared, and then each parameter turns into a variable declaration on the stack. When the function is called, any arguments will be evaluated into the first words onto the stack. As opposed to structure nodes, expression nodes are given locations to leave the expression result, be it a buffer or a location in RAM. This is how assignments work; all they do is pass the variable’s location along to their expressions. When the location is in RAM, the program assumes that the address buffer will be left at that location after the expression finishes. Constants are the simplest expression nodes. They turn into SET instructions. Variables are much more complicated. Each time the program wants to access a variable on the stack, it must add the stack offset to the variable’s position relative to it, and set the address buffer to the result. When looking at the instructions generated by my compiler, most of the computer’s work happens to be shuffling buffers for this purpose. If I were building the computer over again, I’d change the address buffer to be two buffers connected by a full adder. That way, one buffer could contain the stack offset, cutting out ~70% of each program’s instructions. Allowing one of those buffers to be read from would improve it even more, providing a more elegant way to increase or decrease the stack offset. While parsing binary expressions, an additional offset on top of the stack offset is incremented to allow words to be temporarily stored in memory, if they need to be. The following flowchart describes the optimizations done to a binary expression (if possible) The program also introduces two operators for clamping values from 16 bit integer range to a 1 or 0. Essentially, a ‘truthiness’ test, and an inverse truthiness test. The program also supports reference and dereference operators. Pointers are the same data type as any other word, and there are no protections to ensure that the machine doesn’t change memory in use by the OS. Reference operators are are turned into constants. Function calls must add onto the stack offset, leave the current position in instructions in ram, and then evaluate each of its parameter expressions into the stack variables they correspond to inside the function’s scope. After that, the function jumps to the function’s position in the instructions. Once the function reaches its return statement, it subtracts from the stack and leaves the function’s return value in the C buffer. I was originally going to implement a heap with memory allocation and reallocation operators, but I realized that each one would turn into a lot of instructions and ram usage. The computer’s ALU occasionally mishandles operations, so small programs will execute fine, but any program over 80 operations isn’t worth trying. I spent a long time trying to fix that problem, because it would allow me to create a whole OS for the system, but the project got pushed to the wayside when summer ended.</summary></entry><entry><title type="html">Breadboard computer ALU</title><link href="http://localhost:4000/breadboard/2018/04/30/breadboard-computer-alu.html" rel="alternate" type="text/html" title="Breadboard computer ALU" /><published>2018-04-30T10:39:54-04:00</published><updated>2018-04-30T10:39:54-04:00</updated><id>http://localhost:4000/breadboard/2018/04/30/breadboard-computer-alu</id><content type="html" xml:base="http://localhost:4000/breadboard/2018/04/30/breadboard-computer-alu.html">&lt;p&gt;
The alu is the largest part of the computer. Its purpose is to perform binary operations on buffers A and B. To support all of these different operations across a reasonable amount of board space, an effective strategy can be used. The eight operations are arranged in such a way that the rightmost operation selection bit always involves inverting buffer B before performing one of the four binary operations. The rightmost bit is also the full adder’s carry in, which implements subtraction via two’s compliment. The other two operation selection bits select which operation to perform after conditional inversion of B.
&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/breadboard/chapter3/figure1.png&quot; alt=&quot;figure 1&quot; /&gt;&lt;/p&gt;
&lt;p&gt;
While bitwise or, and, xor, and even full adders come in IC chip form, bitwise shifting must be assembled out of smaller elements. Specifically, many multiplexers. The module shown above labeled ‘shifter’ follows the schematic below. Even the simplified schematic bears explaining- each row of multiplexers may translate the word to the right some amount. The first row translates the bus 1 signal over, the second one translates it 2 over, the third translates it 4 over, and so on. There is one more multiplexer at the end that decides whether to listen to the right side or the left side of the message. If the op input is false, the last multiplexer would listen to the red inputs, which results in left shift behavior. When it is true, however, the last multiplexer would listen to the final inputs in the message. This is coupled with an understood inversion to B that occurs outside of the shifter, so for example a right shift by zero would shift the message as far as possible because the bitwise inversion of zero is each bit held true, and then the final multiplexer would collect the message at the far end, leaving it unchanged. Right shift behavior is correctly achieved when the op input is true.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/breadboard/chapter3/figure2.png&quot; alt=&quot;figure 2&quot; /&gt;&lt;/p&gt;

&lt;div&gt;

&lt;a href=&quot;/breadboard/2018/05/07/custom-language-compiler.html&quot; style=&quot;float:right;&quot;&gt;Custom Language Compiler &amp;#8250;&lt;/a&gt;


&lt;a href=&quot;/breadboard/2018/04/16/breadboard-computer-operations.html&quot; class=&quot;float:left;&quot;&gt;&amp;#8249; Breadboard computer operations&lt;/a&gt;

&lt;/div&gt;</content><author><name></name></author><summary type="html">The alu is the largest part of the computer. Its purpose is to perform binary operations on buffers A and B. To support all of these different operations across a reasonable amount of board space, an effective strategy can be used. The eight operations are arranged in such a way that the rightmost operation selection bit always involves inverting buffer B before performing one of the four binary operations. The rightmost bit is also the full adder’s carry in, which implements subtraction via two’s compliment. The other two operation selection bits select which operation to perform after conditional inversion of B. While bitwise or, and, xor, and even full adders come in IC chip form, bitwise shifting must be assembled out of smaller elements. Specifically, many multiplexers. The module shown above labeled ‘shifter’ follows the schematic below. Even the simplified schematic bears explaining- each row of multiplexers may translate the word to the right some amount. The first row translates the bus 1 signal over, the second one translates it 2 over, the third translates it 4 over, and so on. There is one more multiplexer at the end that decides whether to listen to the right side or the left side of the message. If the op input is false, the last multiplexer would listen to the red inputs, which results in left shift behavior. When it is true, however, the last multiplexer would listen to the final inputs in the message. This is coupled with an understood inversion to B that occurs outside of the shifter, so for example a right shift by zero would shift the message as far as possible because the bitwise inversion of zero is each bit held true, and then the final multiplexer would collect the message at the far end, leaving it unchanged. Right shift behavior is correctly achieved when the op input is true.</summary></entry></feed>